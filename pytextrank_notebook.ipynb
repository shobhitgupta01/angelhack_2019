{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1:\n",
    "Perform statistical parsing/tagging on a document in JSON format\n",
    "\n",
    "INPUTS: JSON doc for the text input  \n",
    "OUTPUT: JSON format `ParsedGraf(id, sha1, graf)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from cmd_pytextrank import pytextrank\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_terms(text_doc):\n",
    "    path_stage1 = \"o1.json\"\n",
    "    with open(path_stage1, 'w') as f:\n",
    "        for graf in pytextrank.parse_doc(pytextrank.json_iter(text_doc)):\n",
    "            f.write(\"%s\\n\" % pytextrank.pretty_print(graf._asdict()))\n",
    "\n",
    "    graph, ranks = pytextrank.text_rank(path_stage1)\n",
    "    pytextrank.render_ranks(graph, ranks)\n",
    "\n",
    "    result = []\n",
    "    count = 0;\n",
    "    for rl in pytextrank.normalize_key_phrases(path_stage1, ranks):\n",
    "        count += 1\n",
    "#         if count>5: break\n",
    "#         print(pytextrank.pretty_print(rl))\n",
    "        result.append(ast.literal_eval(pytextrank.pretty_print(rl))[:2])\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['systems', 0.13960200511496842],\n",
       " ['mixed types', 0.10011369334431029],\n",
       " ['minimal set', 0.07722530005001839],\n",
       " ['strict inequations', 0.05932564716688539],\n",
       " ['considered', 0.051510113485796795],\n",
       " ['types', 0.050056846672155146],\n",
       " ['natural numbers', 0.040027127748498155],\n",
       " ['set', 0.038612650025009194],\n",
       " ['minimal generating sets', 0.038612650025009194],\n",
       " ['solutions', 0.03811818160524005],\n",
       " ['linear constraints', 0.034876696539641656],\n",
       " ['linear diophantine equations', 0.031807737038206586],\n",
       " ['solving', 0.031097418010196463],\n",
       " ['inequations', 0.029662823583442695],\n",
       " ['nonstrict inequations', 0.029662823583442695],\n",
       " ['numbers', 0.020013563874249077],\n",
       " ['given', 0.019680155686548313],\n",
       " ['equations', 0.015903868519103293],\n",
       " ['constraints', 0.015786843921044583],\n",
       " ['diophantine', 0.015786843921044583],\n",
       " ['algorithms', 0.014930232257005165],\n",
       " ['supporting', 0.013237566152796207],\n",
       " ['constructing', 0.013175203924198653],\n",
       " ['nonstrict', 0.012657885375219537],\n",
       " ['generating', 0.012142707011244293],\n",
       " ['upper bounds', 0.01212282696116469],\n",
       " ['components', 0.011213619318097083],\n",
       " ['construction', 0.010599101005441286],\n",
       " ['compatibility', 0.006061413480582345],\n",
       " ['bounds', 0.006061413480582345],\n",
       " ['corresponding', 0.006061413480582345],\n",
       " ['criteria', 0.004253627638275056]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_doc = \"./test_data/mih.json\"\n",
    "get_key_terms(text_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# path_stage0 = \"mih.json\"\n",
    "# path_stage1 = \"o1.json\"\n",
    "\n",
    "# with open(path_stage1, 'w') as f:\n",
    "#     for graf in pytextrank.parse_doc(pytextrank.json_iter(path_stage0)):\n",
    "#         f.write(\"%s\\n\" % pytextrank.pretty_print(graf._asdict()))\n",
    "#         # to view output in this notebook\n",
    "# #         print(pytextrank.pretty_print(graf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2:\n",
    "Collect and normalize the key phrases from a parsed document\n",
    "\n",
    "INPUTS: `<stage1>`  \n",
    "OUTPUT: JSON format `RankedLexeme(text, rank, ids, pos)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_stage1 = \"o1.json\"\n",
    "\n",
    "# graph, ranks = pytextrank.text_rank(path_stage1)\n",
    "# pytextrank.render_ranks(graph, ranks)\n",
    "\n",
    "# count = 0;\n",
    "# for rl in pytextrank.normalize_key_phrases(path_stage1, ranks):\n",
    "#     count += 1\n",
    "#     if count>5: break\n",
    "#     print(ast.literal_eval(pytextrank.pretty_print(rl))[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import pylab as plt\n",
    "\n",
    "# nx.draw(graph, with_labels=True) \n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3:\n",
    "Calculate a significance weight for each sentence, using MinHash to approximate a Jaccard distance from key phrases determined by TextRank\n",
    "\n",
    "INPUTS: `<stage1> <stage2>`  \n",
    "OUTPUT: JSON format `SummarySent(dist, idx, text)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_stage1 = \"o1.json\"\n",
    "# path_stage2 = \"o2.json\"\n",
    "# path_stage3 = \"o3.json\"\n",
    "\n",
    "# kernel = pytextrank.rank_kernel(path_stage2)\n",
    "\n",
    "# with open(path_stage3, 'w') as f:\n",
    "#     for s in pytextrank.top_sentences(kernel, path_stage1):\n",
    "#         f.write(pytextrank.pretty_print(s._asdict()))\n",
    "#         f.write(\"\\n\")\n",
    "#         # to view output in this notebook\n",
    "#         print(pytextrank.pretty_print(s._asdict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 4:\n",
    "Summarize a document based on most significant sentences and key phrases\n",
    "\n",
    "INPUTS: `<stage2> <stage3>`  \n",
    "OUTPUT: Markdown format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_stage2 = \"o2.json\"\n",
    "# path_stage3 = \"o3.json\"\n",
    "\n",
    "# p\n",
    "# phrases = \", \".join(set([p for p in pytextrank.limit_keyphrases(path_stage2, phrase_limit=4)]))\n",
    "# # sent_iter = sorted(pytextrank.limit_sentences(path_stage3, word_limit=150), key=lambda x: x[1])\n",
    "# # s = []\n",
    "\n",
    "# # for sent_text, idx in sent_iter:\n",
    "# #     s.append(pytextrank.make_sentence(sent_text))\n",
    "\n",
    "# # graf_text = \" \".join(s)\n",
    "# # print(\"**excerpts:** %s\\n\\n**keywords:** %s\" % (graf_text, phrases,))\n",
    "# print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
